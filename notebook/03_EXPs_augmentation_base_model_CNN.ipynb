{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f451bee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /itf-fi-ml/home/arunps/Projects/VisionInfantNet\n"
     ]
    }
   ],
   "source": [
    "#one level up into project folder\n",
    "import os\n",
    "#os.chdir(\"..\")\n",
    "\n",
    "#print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "818341d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import mlflow\n",
    "import dagshub\n",
    "\n",
    "from visioninfantnet.utils.ml_utils.metric.classification_metric import (\n",
    "    get_classification_score,\n",
    ")\n",
    "from visioninfantnet.exception.exception import VisionInfantNetException\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ad40d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as arunps12\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as arunps12\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"arunps12/VisionInfantNet\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"arunps12/VisionInfantNet\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository arunps12/VisionInfantNet initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository arunps12/VisionInfantNet initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run able-shrike-366 at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/0/runs/fbe01f7dda864da4a81e1941a8c86994\n",
      "üß™ View experiment at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "mlflow_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "mlflow.set_tracking_uri(mlflow_uri)\n",
    "dagshub.init(repo_owner='arunps12', repo_name='VisionInfantNet', mlflow=True)\n",
    "\n",
    "import mlflow\n",
    "with mlflow.start_run():\n",
    "  mlflow.log_param('parameter name', 'value')\n",
    "  mlflow.log_metric('metric name', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24484568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/itf-fi-ml/home/arunps/Projects/VisionInfantNet/artifacts/12_02_2025_10_27_46/data_transformation/spectrograms/train\n",
      "/itf-fi-ml/home/arunps/Projects/VisionInfantNet/artifacts/12_02_2025_10_27_46/data_transformation/spectrograms/valid\n",
      "/itf-fi-ml/home/arunps/Projects/VisionInfantNet/artifacts/12_02_2025_10_27_46/data_transformation/features/train_labels.npy\n",
      "/itf-fi-ml/home/arunps/Projects/VisionInfantNet/artifacts/12_02_2025_10_27_46/data_transformation/features/valid_labels.npy\n"
     ]
    }
   ],
   "source": [
    "#Paths and data loading helper\n",
    "\n",
    "ARTIFACT_ROOT = os.getenv(\"ARTIFACT_ROOT\")\n",
    "\n",
    "TRAIN_IMG_DIR = os.path.join(ARTIFACT_ROOT, \"data_transformation\", \"spectrograms\", \"train\")\n",
    "VAL_IMG_DIR   = os.path.join(ARTIFACT_ROOT, \"data_transformation\", \"spectrograms\", \"valid\")\n",
    "\n",
    "TRAIN_LABEL_NPY = os.path.join(ARTIFACT_ROOT, \"data_transformation\", \"features\", \"train_labels.npy\")\n",
    "VAL_LABEL_NPY   = os.path.join(ARTIFACT_ROOT, \"data_transformation\", \"features\", \"valid_labels.npy\")\n",
    "\n",
    "print(TRAIN_IMG_DIR)\n",
    "print(VAL_IMG_DIR)\n",
    "print(TRAIN_LABEL_NPY)\n",
    "print(VAL_LABEL_NPY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3778f318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600 3600\n",
      "3580 3580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load labels and image paths\n",
    "train_labels = np.load(TRAIN_LABEL_NPY)\n",
    "val_labels   = np.load(VAL_LABEL_NPY)\n",
    "\n",
    "train_image_paths = sorted(glob.glob(os.path.join(TRAIN_IMG_DIR, \"*.png\")))\n",
    "val_image_paths   = sorted(glob.glob(os.path.join(VAL_IMG_DIR, \"*.png\")))\n",
    "\n",
    "print(len(train_image_paths), len(train_labels))\n",
    "print(len(val_image_paths), len(val_labels))\n",
    "\n",
    "num_classes = len(np.unique(train_labels))\n",
    "num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b48fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {np.str_('Canonical'): 0, np.str_('Crying'): 1, np.str_('Junk'): 2, np.str_('Laughing'): 3, np.str_('Non-canonical'): 4}\n",
      "num_classes: 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unique_labels = sorted(set(train_labels) | set(val_labels))\n",
    "label_to_idx = {lab: i for i, lab in enumerate(unique_labels)}\n",
    "idx_to_label = {i: lab for lab, i in label_to_idx.items()}\n",
    "\n",
    "print(\"Label mapping:\", label_to_idx)\n",
    "num_classes = len(unique_labels)\n",
    "print(\"num_classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e24f1898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "train_labels_idx = np.array([label_to_idx[l] for l in train_labels])\n",
    "val_labels_idx   = np.array([label_to_idx[l] for l in val_labels])\n",
    "print(train_labels_idx[:10])\n",
    "print(val_labels_idx[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb230b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [ 444  243 1430   46 1437]\n",
      "Class weights (per class): [0.00225225 0.00411523 0.0006993  0.02173913 0.00069589]\n",
      "Sample weights shape: torch.Size([3600])\n",
      "Sample weights (first 10): tensor([0.0041, 0.0041, 0.0041, 0.0041, 0.0041, 0.0041, 0.0041, 0.0041, 0.0041,\n",
      "        0.0041])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# counts per class in TRAIN set\n",
    "class_counts = np.bincount(train_labels_idx, minlength=num_classes)\n",
    "print(\"Class counts:\", class_counts)\n",
    "\n",
    "# inverse-frequency weights ‚Üí higher for minority classes\n",
    "class_weights = 1.0 / (class_counts + 1e-6)\n",
    "print(\"Class weights (per class):\", class_weights)\n",
    "\n",
    "# per-sample weights\n",
    "sample_weights = class_weights[train_labels_idx]     # shape: [num_train_samples]\n",
    "sample_weights_tensor = torch.from_numpy(sample_weights).float()\n",
    "print(\"Sample weights shape:\", sample_weights_tensor.shape)\n",
    "print(\"Sample weights (first 10):\", sample_weights_tensor[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "432ef013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "def _get_ft_axes(spec: torch.Tensor):\n",
    "    \"\"\"\n",
    "    For image/spectrogram tensors shaped [C,H,W] or [H,W].\n",
    "    Treat H as 'freq' and W as 'time'.\n",
    "    \"\"\"\n",
    "    if spec.ndim == 3:   # [C, H, W]\n",
    "        return 1, 2\n",
    "    elif spec.ndim == 2: # [H, W]\n",
    "        return 0, 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected tensor shape: {spec.shape}\")\n",
    "\n",
    "def random_time_mask(spec, max_mask_pct=0.1, num_masks=1):\n",
    "    \"\"\"\n",
    "    Mask along the time axis (width).\n",
    "    \"\"\"\n",
    "    F_axis, T_axis = _get_ft_axes(spec)\n",
    "    _, T = (spec.shape[F_axis], spec.shape[T_axis])\n",
    "    out = spec.clone()\n",
    "    max_mask = int(T * max_mask_pct)\n",
    "    if max_mask < 1:\n",
    "        return out\n",
    "    for _ in range(num_masks):\n",
    "        t = random.randint(0, max_mask)\n",
    "        t0 = random.randint(0, max(0, T - t))\n",
    "        idx = torch.arange(t0, t0 + t, device=spec.device)\n",
    "        out.index_fill_(T_axis, idx, 0.0)\n",
    "    return out\n",
    "\n",
    "def random_freq_mask(spec, max_mask_pct=0.1, num_masks=1):\n",
    "    \"\"\"\n",
    "    Mask along the frequency axis (height).\n",
    "    \"\"\"\n",
    "    F_axis, T_axis = _get_ft_axes(spec)\n",
    "    F_dim, _ = (spec.shape[F_axis], spec.shape[T_axis])\n",
    "    out = spec.clone()\n",
    "    max_mask = int(F_dim * max_mask_pct)\n",
    "    if max_mask < 1:\n",
    "        return out\n",
    "    for _ in range(num_masks):\n",
    "        f = random.randint(0, max_mask)\n",
    "        f0 = random.randint(0, max(0, F_dim - f))\n",
    "        idx = torch.arange(f0, f0 + f, device=spec.device)\n",
    "        out.index_fill_(F_axis, idx, 0.0)\n",
    "    return out\n",
    "\n",
    "def random_time_shift(spec, max_shift_pct=0.1):\n",
    "    \"\"\"\n",
    "    Roll along time axis.\n",
    "    \"\"\"\n",
    "    F_axis, T_axis = _get_ft_axes(spec)\n",
    "    _, T = (spec.shape[F_axis], spec.shape[T_axis])\n",
    "    max_shift = int(T * max_shift_pct)\n",
    "    if max_shift < 1:\n",
    "        return spec\n",
    "    shift = random.randint(-max_shift, max_shift)\n",
    "    return torch.roll(spec, shifts=shift, dims=T_axis)\n",
    "\n",
    "def random_gain(spec, min_gain=0.8, max_gain=1.2):\n",
    "    \"\"\"\n",
    "    Multiply tensor by a random gain factor.\n",
    "    \"\"\"\n",
    "    gain = random.uniform(min_gain, max_gain)\n",
    "    return spec * gain\n",
    "\n",
    "def apply_augmentations(spec, label, aug_cfg: dict):\n",
    "    \"\"\"\n",
    "    Apply a combination of augmentations defined in aug_cfg to spec.\n",
    "    \"\"\"\n",
    "    if aug_cfg.get(\"time_mask\", False):\n",
    "        spec = random_time_mask(\n",
    "            spec,\n",
    "            max_mask_pct=aug_cfg.get(\"time_mask_pct\", 0.1),\n",
    "            num_masks=aug_cfg.get(\"time_mask_num\", 1),\n",
    "        )\n",
    "\n",
    "    if aug_cfg.get(\"freq_mask\", False):\n",
    "        spec = random_freq_mask(\n",
    "            spec,\n",
    "            max_mask_pct=aug_cfg.get(\"freq_mask_pct\", 0.1),\n",
    "            num_masks=aug_cfg.get(\"freq_mask_num\", 1),\n",
    "        )\n",
    "\n",
    "    if aug_cfg.get(\"time_shift\", False):\n",
    "        spec = random_time_shift(\n",
    "            spec,\n",
    "            max_shift_pct=aug_cfg.get(\"time_shift_pct\", 0.1),\n",
    "        )\n",
    "\n",
    "    if aug_cfg.get(\"gain\", False):\n",
    "        spec = random_gain(\n",
    "            spec,\n",
    "            min_gain=aug_cfg.get(\"gain_min\", 0.8),\n",
    "            max_gain=aug_cfg.get(\"gain_max\", 1.2),\n",
    "        )\n",
    "\n",
    "    return spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b99998b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "class SpectrogramImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, train: bool = True, aug_cfg: dict = None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.train = train\n",
    "        self.aug_cfg = aug_cfg or {}\n",
    "\n",
    "        base_transforms = [\n",
    "            T.Resize((224, 224)),\n",
    "            T.ToTensor(),              # -> [1, H, W], values [0,1]\n",
    "        ]\n",
    "        self.transforms = T.Compose(base_transforms)\n",
    "\n",
    "        # Normalize to [-1,1] per channel\n",
    "        self.normalize = T.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5],\n",
    "            std=[0.5, 0.5, 0.5],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        # Load grayscale PNG (0..255)\n",
    "        img = Image.open(path).convert(\"L\")\n",
    "\n",
    "        # Apply transforms ‚Üí [1,224,224] in [0,1]\n",
    "        img = self.transforms(img)\n",
    "\n",
    "        # Convert grayscale -> RGB by repeating channels ‚Üí [3,224,224]\n",
    "        img = img.repeat(3, 1, 1)\n",
    "\n",
    "        # Apply augmentations only for training\n",
    "        if self.train and self.aug_cfg:\n",
    "            img = apply_augmentations(img, label, self.aug_cfg)\n",
    "\n",
    "        # Normalize to [-1,1]\n",
    "        img = self.normalize(img)\n",
    "\n",
    "        return img, torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a61b8d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mixup_batch(inputs, labels, alpha=0.4):\n",
    "    \"\"\"\n",
    "    MixUp for one batch.\n",
    "    Returns mixed_inputs, targets_a, targets_b, lam.\n",
    "    \"\"\"\n",
    "    if alpha <= 0:\n",
    "        return inputs, labels, labels, 1.0\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = inputs.size(0)\n",
    "    index = torch.randperm(batch_size, device=inputs.device)\n",
    "\n",
    "    mixed_inputs = lam * inputs + (1 - lam) * inputs[index]\n",
    "    targets_a, targets_b = labels, labels[index]\n",
    "    return mixed_inputs, targets_a, targets_b, lam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "131e87b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # downsample by /2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class BaseCNNSpectrogram(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        base_channels: int = 32,\n",
    "        num_blocks: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = [3] + [base_channels * (2 ** i) for i in range(num_blocks)]\n",
    "        conv_blocks = []\n",
    "        for i in range(num_blocks):\n",
    "            conv_blocks.append(ConvBlock(channels[i], channels[i+1]))\n",
    "        self.conv = nn.Sequential(*conv_blocks)\n",
    "\n",
    "        last_channels = channels[-1]\n",
    "        spatial_size = 224 // (2 ** num_blocks)  # after MaxPool(2) num_blocks times\n",
    "        self.flatten_dim = last_channels * spatial_size * spatial_size\n",
    "\n",
    "        # Single DNN classification layer: Flatten -> Linear -> logits\n",
    "        self.fc = nn.Linear(self.flatten_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f96e6ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "import torch.nn as nn\n",
    "\n",
    "def init_cnn_weights(m, init_method=\"pytorch_default\"):\n",
    "    \"\"\"\n",
    "    Apply chosen initialization method to Conv and Linear layers.\n",
    "    For 'pytorch_default' we do nothing.\n",
    "    \"\"\"\n",
    "    if not isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        return\n",
    "\n",
    "    if init_method == \"pytorch_default\":\n",
    "        return  # keep PyTorch default\n",
    "\n",
    "    if init_method == \"kaiming_normal\":\n",
    "        init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "    elif init_method == \"kaiming_uniform\":\n",
    "        init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n",
    "    elif init_method == \"xavier_normal\":\n",
    "        init.xavier_normal_(m.weight)\n",
    "    elif init_method == \"xavier_uniform\":\n",
    "        init.xavier_uniform_(m.weight)\n",
    "\n",
    "    if m.bias is not None:\n",
    "        nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf53fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLflow experiment: cnn_exp4_augment_ablation\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 15\n",
    "\n",
    "NUM_BLOCKS = 5\n",
    "BASE_CHANNELS = 64\n",
    "INIT_METHOD = \"pytorch_default\"\n",
    "\n",
    "MLFLOW_EXPERIMENT = \"cnn_exp4_augment_ablation\"\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT)\n",
    "print(\"Using MLflow experiment:\", MLFLOW_EXPERIMENT)\n",
    "\n",
    "def train_one_cnn_experiment(\n",
    "    num_blocks: int,\n",
    "    base_channels: int,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    num_classes: int,\n",
    "    batch_size: int = 64,\n",
    "    lr: float = 1e-3,\n",
    "    weight_decay: float = 1e-4,\n",
    "    epochs: int = 15,\n",
    "    run_name: str = None,\n",
    "    MODEL_DIR: str = \"saved_models\",\n",
    "    init_method: str = \"pytorch_default\",\n",
    "    use_mixup: bool = False,\n",
    "    mixup_alpha: float = 0.4,\n",
    "    aug_cfg_name: str = \"none\",\n",
    "    use_weighted_sampler: bool = False,\n",
    "    sample_weights: torch.Tensor = None,\n",
    "):\n",
    "    if use_weighted_sampler and sample_weights is not None:\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=sample_weights,\n",
    "            num_samples=len(sample_weights),\n",
    "            replacement=True,\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "    else:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model = BaseCNNSpectrogram(\n",
    "        num_classes=num_classes,\n",
    "        base_channels=base_channels,\n",
    "        num_blocks=num_blocks,\n",
    "    ).to(device)\n",
    "\n",
    "    model.apply(lambda m: init_cnn_weights(m, init_method=init_method))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "    if run_name is None:\n",
    "        run_name = f\"cnn_blocks{num_blocks}_base{base_channels}_{init_method}_{aug_cfg_name}\"\n",
    "\n",
    "    best_val_uar = 0.0\n",
    "    best_epoch = -1\n",
    "\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_param(\"num_blocks\", num_blocks)\n",
    "        mlflow.log_param(\"base_channels\", base_channels)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "        mlflow.log_param(\"weight_decay\", weight_decay)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"model_type\", \"BaseCNN_flatten_linear\")\n",
    "        mlflow.log_param(\"init_method\", init_method)\n",
    "        mlflow.log_param(\"use_mixup\", use_mixup)\n",
    "        mlflow.log_param(\"mixup_alpha\", mixup_alpha)\n",
    "        mlflow.log_param(\"aug_cfg_name\", aug_cfg_name)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # ----- TRAIN -----\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "\n",
    "            for imgs, labels in train_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if use_mixup:\n",
    "                    imgs_mixed, targets_a, targets_b, lam = mixup_batch(\n",
    "                        imgs, labels, alpha=mixup_alpha\n",
    "                    )\n",
    "                    outputs = model(imgs_mixed)\n",
    "                    loss = lam * criterion(outputs, targets_a) + \\\n",
    "                           (1 - lam) * criterion(outputs, targets_b)\n",
    "                else:\n",
    "                    outputs = model(imgs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item() * imgs.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                train_correct += (preds == labels).sum().item()\n",
    "                train_total += labels.size(0)\n",
    "\n",
    "            train_loss /= train_total\n",
    "            train_acc = train_correct / train_total\n",
    "\n",
    "            # ----- VALIDATION -----\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            all_true, all_pred = [], []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for imgs, labels in val_loader:\n",
    "                    imgs = imgs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    outputs = model(imgs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    val_loss += loss.item() * imgs.size(0)\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "\n",
    "                    val_correct += (preds == labels).sum().item()\n",
    "                    val_total += labels.size(0)\n",
    "\n",
    "                    all_true.extend(labels.cpu().numpy().tolist())\n",
    "                    all_pred.extend(preds.cpu().numpy().tolist())\n",
    "\n",
    "            val_loss /= val_total\n",
    "            val_acc = val_correct / val_total\n",
    "\n",
    "            metrics = get_classification_score(\n",
    "                y_true=all_true,\n",
    "                y_pred=all_pred,\n",
    "                average=\"weighted\",\n",
    "            )\n",
    "\n",
    "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"train_acc\", train_acc, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_acc\", val_acc, step=epoch)\n",
    "            mlflow.log_metric(\"val_f1\", metrics.f1_score, step=epoch)\n",
    "            mlflow.log_metric(\"val_precision\", metrics.precision_score, step=epoch)\n",
    "            mlflow.log_metric(\"val_recall\", metrics.recall_score, step=epoch)\n",
    "            mlflow.log_metric(\"val_uar\", metrics.uar, step=epoch)\n",
    "\n",
    "            if metrics.uar > best_val_uar:\n",
    "                best_val_uar = metrics.uar\n",
    "                best_epoch = epoch\n",
    "                model_path = os.path.join(\n",
    "                    MODEL_DIR,\n",
    "                    f\"best_model_blocks{num_blocks}_base{base_channels}_init_{init_method}_aug_{aug_cfg_name}.pt\"\n",
    "                )\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                mlflow.log_artifact(model_path)\n",
    "\n",
    "            print(\n",
    "                f\"[{run_name}] Epoch {epoch+1}/{epochs} | \"\n",
    "                f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.3f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.3f} | \"\n",
    "                f\"F1: {metrics.f1_score:.3f} UAR: {metrics.uar:.3f}\"\n",
    "            )\n",
    "\n",
    "        mlflow.log_metric(\"best_val_uar\", best_val_uar)\n",
    "        mlflow.log_param(\"best_epoch\", best_epoch)\n",
    "\n",
    "    return model, best_val_uar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a4d71ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_AUG_CFG = {\n",
    "    \"time_mask\": True,\n",
    "    \"time_mask_pct\": 0.1,\n",
    "    \"time_mask_num\": 2,\n",
    "    \"freq_mask\": True,\n",
    "    \"freq_mask_pct\": 0.1,\n",
    "    \"freq_mask_num\": 2,\n",
    "    \"time_shift\": True,\n",
    "    \"time_shift_pct\": 0.1,\n",
    "    \"gain\": True,\n",
    "    \"gain_min\": 0.8,\n",
    "    \"gain_max\": 1.2,\n",
    "}\n",
    "\n",
    "AUG_EXPERIMENTS = {\n",
    "    \n",
    "    \"all_aug\": {\n",
    "        \"aug_cfg\": FULL_AUG_CFG,\n",
    "        \"use_mixup\": True,\n",
    "    },\n",
    "    \"no_time_mask\": {\n",
    "        \"aug_cfg\": {**FULL_AUG_CFG, \"time_mask\": False},\n",
    "        \"use_mixup\": True,\n",
    "    },\n",
    "    \"no_freq_mask\": {\n",
    "        \"aug_cfg\": {**FULL_AUG_CFG, \"freq_mask\": False},\n",
    "        \"use_mixup\": True,\n",
    "    },\n",
    "    \"no_time_shift\": {\n",
    "        \"aug_cfg\": {**FULL_AUG_CFG, \"time_shift\": False},\n",
    "        \"use_mixup\": True,\n",
    "    },\n",
    "    \"no_gain\": {\n",
    "        \"aug_cfg\": {**FULL_AUG_CFG, \"gain\": False},\n",
    "        \"use_mixup\": True,\n",
    "    },\n",
    "    \"no_mixup\": {\n",
    "        \"aug_cfg\": FULL_AUG_CFG,\n",
    "        \"use_mixup\": False,\n",
    "    },\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \"no_aug\": {\n",
    "        \"aug_cfg\": {},      \n",
    "        \"use_mixup\": False,\n",
    "    },\n",
    "\n",
    "    \n",
    "    \"mixup_only\": {\n",
    "        \"aug_cfg\": {\n",
    "            \"time_mask\": False,\n",
    "            \"freq_mask\": False,\n",
    "            \"time_shift\": False,\n",
    "            \"gain\": False,\n",
    "        },\n",
    "        \"use_mixup\": True,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba03f77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running augmentation config: all_aug ===\n",
      "[aug_all_aug_blocks5_base64] Epoch 1/30 | Train Loss: 7.1946 Acc: 0.204 | Val Loss: 2.3000 Acc: 0.288 | F1: 0.298 UAR: 0.243\n",
      "[aug_all_aug_blocks5_base64] Epoch 2/30 | Train Loss: 2.8983 Acc: 0.217 | Val Loss: 6.2008 Acc: 0.089 | F1: 0.041 UAR: 0.225\n",
      "[aug_all_aug_blocks5_base64] Epoch 3/30 | Train Loss: 2.8181 Acc: 0.219 | Val Loss: 3.4461 Acc: 0.149 | F1: 0.151 UAR: 0.214\n",
      "[aug_all_aug_blocks5_base64] Epoch 4/30 | Train Loss: 2.9279 Acc: 0.214 | Val Loss: 13.0015 Acc: 0.185 | F1: 0.216 UAR: 0.215\n",
      "[aug_all_aug_blocks5_base64] Epoch 5/30 | Train Loss: 2.6335 Acc: 0.213 | Val Loss: 2.1936 Acc: 0.276 | F1: 0.215 UAR: 0.242\n",
      "[aug_all_aug_blocks5_base64] Epoch 6/30 | Train Loss: 2.1508 Acc: 0.239 | Val Loss: 1.6617 Acc: 0.331 | F1: 0.343 UAR: 0.236\n",
      "[aug_all_aug_blocks5_base64] Epoch 7/30 | Train Loss: 1.8889 Acc: 0.228 | Val Loss: 1.8545 Acc: 0.182 | F1: 0.184 UAR: 0.237\n",
      "[aug_all_aug_blocks5_base64] Epoch 8/30 | Train Loss: 1.7237 Acc: 0.235 | Val Loss: 1.9583 Acc: 0.242 | F1: 0.223 UAR: 0.239\n",
      "[aug_all_aug_blocks5_base64] Epoch 9/30 | Train Loss: 1.6864 Acc: 0.246 | Val Loss: 1.5204 Acc: 0.307 | F1: 0.286 UAR: 0.224\n",
      "[aug_all_aug_blocks5_base64] Epoch 10/30 | Train Loss: 1.6434 Acc: 0.258 | Val Loss: 1.4654 Acc: 0.360 | F1: 0.338 UAR: 0.236\n",
      "[aug_all_aug_blocks5_base64] Epoch 11/30 | Train Loss: 1.5977 Acc: 0.258 | Val Loss: 1.8766 Acc: 0.120 | F1: 0.141 UAR: 0.237\n",
      "[aug_all_aug_blocks5_base64] Epoch 12/30 | Train Loss: 1.5266 Acc: 0.300 | Val Loss: 1.4733 Acc: 0.329 | F1: 0.315 UAR: 0.225\n",
      "[aug_all_aug_blocks5_base64] Epoch 13/30 | Train Loss: 1.5311 Acc: 0.263 | Val Loss: 1.7696 Acc: 0.251 | F1: 0.261 UAR: 0.241\n",
      "[aug_all_aug_blocks5_base64] Epoch 14/30 | Train Loss: 1.4779 Acc: 0.307 | Val Loss: 1.8328 Acc: 0.188 | F1: 0.172 UAR: 0.243\n",
      "[aug_all_aug_blocks5_base64] Epoch 15/30 | Train Loss: 1.4846 Acc: 0.284 | Val Loss: 1.9041 Acc: 0.186 | F1: 0.182 UAR: 0.246\n",
      "[aug_all_aug_blocks5_base64] Epoch 16/30 | Train Loss: 1.4454 Acc: 0.319 | Val Loss: 1.6969 Acc: 0.298 | F1: 0.235 UAR: 0.245\n",
      "[aug_all_aug_blocks5_base64] Epoch 17/30 | Train Loss: 1.4211 Acc: 0.325 | Val Loss: 1.5086 Acc: 0.283 | F1: 0.267 UAR: 0.240\n",
      "[aug_all_aug_blocks5_base64] Epoch 18/30 | Train Loss: 1.4406 Acc: 0.300 | Val Loss: 1.3732 Acc: 0.353 | F1: 0.371 UAR: 0.237\n",
      "[aug_all_aug_blocks5_base64] Epoch 19/30 | Train Loss: 1.4082 Acc: 0.298 | Val Loss: 1.4910 Acc: 0.297 | F1: 0.262 UAR: 0.240\n",
      "[aug_all_aug_blocks5_base64] Epoch 20/30 | Train Loss: 1.4149 Acc: 0.326 | Val Loss: 1.5623 Acc: 0.275 | F1: 0.303 UAR: 0.229\n",
      "[aug_all_aug_blocks5_base64] Epoch 21/30 | Train Loss: 1.3744 Acc: 0.321 | Val Loss: 1.4504 Acc: 0.325 | F1: 0.351 UAR: 0.236\n",
      "[aug_all_aug_blocks5_base64] Epoch 22/30 | Train Loss: 1.3476 Acc: 0.349 | Val Loss: 1.5795 Acc: 0.268 | F1: 0.295 UAR: 0.252\n",
      "[aug_all_aug_blocks5_base64] Epoch 23/30 | Train Loss: 1.3552 Acc: 0.334 | Val Loss: 1.7604 Acc: 0.275 | F1: 0.284 UAR: 0.254\n",
      "[aug_all_aug_blocks5_base64] Epoch 24/30 | Train Loss: 1.3366 Acc: 0.329 | Val Loss: 1.6410 Acc: 0.265 | F1: 0.292 UAR: 0.265\n",
      "[aug_all_aug_blocks5_base64] Epoch 25/30 | Train Loss: 1.2840 Acc: 0.358 | Val Loss: 1.5118 Acc: 0.305 | F1: 0.299 UAR: 0.250\n",
      "[aug_all_aug_blocks5_base64] Epoch 26/30 | Train Loss: 1.2702 Acc: 0.347 | Val Loss: 1.6136 Acc: 0.280 | F1: 0.297 UAR: 0.237\n",
      "[aug_all_aug_blocks5_base64] Epoch 27/30 | Train Loss: 1.3322 Acc: 0.345 | Val Loss: 1.4931 Acc: 0.300 | F1: 0.307 UAR: 0.239\n",
      "[aug_all_aug_blocks5_base64] Epoch 28/30 | Train Loss: 1.2792 Acc: 0.347 | Val Loss: 1.6208 Acc: 0.256 | F1: 0.290 UAR: 0.243\n",
      "[aug_all_aug_blocks5_base64] Epoch 29/30 | Train Loss: 1.2656 Acc: 0.365 | Val Loss: 1.4913 Acc: 0.304 | F1: 0.336 UAR: 0.238\n",
      "[aug_all_aug_blocks5_base64] Epoch 30/30 | Train Loss: 1.1993 Acc: 0.376 | Val Loss: 1.4998 Acc: 0.333 | F1: 0.315 UAR: 0.231\n",
      "üèÉ View run aug_all_aug_blocks5_base64 at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4/runs/e3d2790fe317453daaf64fa77ee04f89\n",
      "üß™ View experiment at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4\n",
      "--> all_aug: best UAR=0.2648\n",
      "\n",
      "=== Running augmentation config: no_time_mask ===\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 1/30 | Train Loss: 7.3520 Acc: 0.209 | Val Loss: 3.7619 Acc: 0.135 | F1: 0.182 UAR: 0.226\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 2/30 | Train Loss: 2.8398 Acc: 0.218 | Val Loss: 2.8520 Acc: 0.369 | F1: 0.307 UAR: 0.228\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 3/30 | Train Loss: 2.4697 Acc: 0.229 | Val Loss: 2.8459 Acc: 0.168 | F1: 0.220 UAR: 0.230\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 4/30 | Train Loss: 2.4804 Acc: 0.220 | Val Loss: 2.1299 Acc: 0.189 | F1: 0.222 UAR: 0.223\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 5/30 | Train Loss: 2.5797 Acc: 0.219 | Val Loss: 2.6582 Acc: 0.227 | F1: 0.223 UAR: 0.232\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 6/30 | Train Loss: 2.2961 Acc: 0.240 | Val Loss: 2.6582 Acc: 0.372 | F1: 0.276 UAR: 0.209\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 7/30 | Train Loss: 2.3247 Acc: 0.234 | Val Loss: 2.1029 Acc: 0.336 | F1: 0.285 UAR: 0.226\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 8/30 | Train Loss: 2.0803 Acc: 0.246 | Val Loss: 2.0774 Acc: 0.175 | F1: 0.158 UAR: 0.232\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 9/30 | Train Loss: 2.1333 Acc: 0.241 | Val Loss: 2.8030 Acc: 0.184 | F1: 0.171 UAR: 0.228\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 10/30 | Train Loss: 2.1093 Acc: 0.247 | Val Loss: 1.7319 Acc: 0.352 | F1: 0.355 UAR: 0.240\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 11/30 | Train Loss: 1.8764 Acc: 0.266 | Val Loss: 1.6927 Acc: 0.296 | F1: 0.337 UAR: 0.260\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 12/30 | Train Loss: 1.6838 Acc: 0.255 | Val Loss: 1.9896 Acc: 0.238 | F1: 0.224 UAR: 0.247\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 13/30 | Train Loss: 1.5855 Acc: 0.274 | Val Loss: 1.6254 Acc: 0.301 | F1: 0.326 UAR: 0.250\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 14/30 | Train Loss: 1.5267 Acc: 0.303 | Val Loss: 1.4724 Acc: 0.323 | F1: 0.349 UAR: 0.247\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 15/30 | Train Loss: 1.4554 Acc: 0.285 | Val Loss: 1.3902 Acc: 0.376 | F1: 0.347 UAR: 0.224\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 16/30 | Train Loss: 1.5007 Acc: 0.296 | Val Loss: 1.6414 Acc: 0.268 | F1: 0.226 UAR: 0.257\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 17/30 | Train Loss: 1.3860 Acc: 0.341 | Val Loss: 1.9712 Acc: 0.183 | F1: 0.207 UAR: 0.247\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 18/30 | Train Loss: 1.3885 Acc: 0.345 | Val Loss: 1.5770 Acc: 0.285 | F1: 0.281 UAR: 0.254\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 19/30 | Train Loss: 1.3606 Acc: 0.366 | Val Loss: 1.6772 Acc: 0.245 | F1: 0.263 UAR: 0.252\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 20/30 | Train Loss: 1.4034 Acc: 0.351 | Val Loss: 1.6346 Acc: 0.270 | F1: 0.311 UAR: 0.261\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 21/30 | Train Loss: 1.3219 Acc: 0.361 | Val Loss: 1.6435 Acc: 0.237 | F1: 0.278 UAR: 0.240\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 22/30 | Train Loss: 1.3121 Acc: 0.352 | Val Loss: 1.3805 Acc: 0.338 | F1: 0.318 UAR: 0.219\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 23/30 | Train Loss: 1.3200 Acc: 0.353 | Val Loss: 1.6358 Acc: 0.214 | F1: 0.257 UAR: 0.243\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 24/30 | Train Loss: 1.2738 Acc: 0.371 | Val Loss: 1.7555 Acc: 0.240 | F1: 0.257 UAR: 0.258\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 25/30 | Train Loss: 1.2992 Acc: 0.340 | Val Loss: 1.4865 Acc: 0.302 | F1: 0.325 UAR: 0.256\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 26/30 | Train Loss: 1.2689 Acc: 0.382 | Val Loss: 1.4615 Acc: 0.319 | F1: 0.324 UAR: 0.248\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 27/30 | Train Loss: 1.2590 Acc: 0.359 | Val Loss: 1.7438 Acc: 0.236 | F1: 0.251 UAR: 0.264\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 28/30 | Train Loss: 1.2129 Acc: 0.384 | Val Loss: 1.3918 Acc: 0.356 | F1: 0.363 UAR: 0.239\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 29/30 | Train Loss: 1.2431 Acc: 0.366 | Val Loss: 1.4202 Acc: 0.349 | F1: 0.308 UAR: 0.215\n",
      "[aug_no_time_mask_blocks5_base64] Epoch 30/30 | Train Loss: 1.1813 Acc: 0.426 | Val Loss: 1.5651 Acc: 0.324 | F1: 0.263 UAR: 0.216\n",
      "üèÉ View run aug_no_time_mask_blocks5_base64 at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4/runs/de32f1165608409dbc92d6803c41dfe5\n",
      "üß™ View experiment at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4\n",
      "--> no_time_mask: best UAR=0.2639\n",
      "\n",
      "=== Running augmentation config: no_freq_mask ===\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 1/30 | Train Loss: 7.2779 Acc: 0.207 | Val Loss: 3.2585 Acc: 0.159 | F1: 0.196 UAR: 0.223\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 2/30 | Train Loss: 2.7186 Acc: 0.220 | Val Loss: 2.6023 Acc: 0.209 | F1: 0.233 UAR: 0.262\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 3/30 | Train Loss: 2.4397 Acc: 0.224 | Val Loss: 2.6633 Acc: 0.310 | F1: 0.283 UAR: 0.247\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 4/30 | Train Loss: 2.4057 Acc: 0.229 | Val Loss: 2.0438 Acc: 0.213 | F1: 0.199 UAR: 0.221\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 5/30 | Train Loss: 2.5019 Acc: 0.230 | Val Loss: 2.4438 Acc: 0.283 | F1: 0.249 UAR: 0.227\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 6/30 | Train Loss: 2.3067 Acc: 0.240 | Val Loss: 2.9058 Acc: 0.160 | F1: 0.218 UAR: 0.228\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 7/30 | Train Loss: 2.0159 Acc: 0.240 | Val Loss: 2.4072 Acc: 0.146 | F1: 0.130 UAR: 0.235\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 8/30 | Train Loss: 1.8375 Acc: 0.231 | Val Loss: 2.2969 Acc: 0.149 | F1: 0.137 UAR: 0.229\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 9/30 | Train Loss: 1.7735 Acc: 0.243 | Val Loss: 2.2114 Acc: 0.111 | F1: 0.127 UAR: 0.248\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 10/30 | Train Loss: 1.6441 Acc: 0.269 | Val Loss: 1.4787 Acc: 0.323 | F1: 0.347 UAR: 0.216\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 11/30 | Train Loss: 1.6270 Acc: 0.260 | Val Loss: 1.6966 Acc: 0.224 | F1: 0.274 UAR: 0.218\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 12/30 | Train Loss: 1.5540 Acc: 0.282 | Val Loss: 1.5856 Acc: 0.267 | F1: 0.259 UAR: 0.225\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 13/30 | Train Loss: 1.5008 Acc: 0.317 | Val Loss: 1.7576 Acc: 0.221 | F1: 0.247 UAR: 0.251\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 14/30 | Train Loss: 1.4407 Acc: 0.307 | Val Loss: 1.6058 Acc: 0.289 | F1: 0.244 UAR: 0.230\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 15/30 | Train Loss: 1.4392 Acc: 0.308 | Val Loss: 1.8303 Acc: 0.197 | F1: 0.242 UAR: 0.236\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 16/30 | Train Loss: 1.4133 Acc: 0.332 | Val Loss: 1.5485 Acc: 0.275 | F1: 0.303 UAR: 0.238\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 17/30 | Train Loss: 1.3441 Acc: 0.340 | Val Loss: 1.7617 Acc: 0.187 | F1: 0.215 UAR: 0.224\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 18/30 | Train Loss: 1.3487 Acc: 0.350 | Val Loss: 1.4824 Acc: 0.353 | F1: 0.246 UAR: 0.216\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 19/30 | Train Loss: 1.3648 Acc: 0.328 | Val Loss: 1.4316 Acc: 0.336 | F1: 0.357 UAR: 0.243\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 20/30 | Train Loss: 1.2927 Acc: 0.392 | Val Loss: 1.7134 Acc: 0.211 | F1: 0.236 UAR: 0.241\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 21/30 | Train Loss: 1.2569 Acc: 0.383 | Val Loss: 1.5411 Acc: 0.269 | F1: 0.283 UAR: 0.246\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 22/30 | Train Loss: 1.2548 Acc: 0.370 | Val Loss: 1.5636 Acc: 0.278 | F1: 0.299 UAR: 0.241\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 23/30 | Train Loss: 1.1920 Acc: 0.373 | Val Loss: 1.6847 Acc: 0.246 | F1: 0.269 UAR: 0.246\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 24/30 | Train Loss: 1.1602 Acc: 0.397 | Val Loss: 1.5096 Acc: 0.304 | F1: 0.334 UAR: 0.216\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 25/30 | Train Loss: 1.1984 Acc: 0.393 | Val Loss: 1.4318 Acc: 0.347 | F1: 0.362 UAR: 0.235\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 26/30 | Train Loss: 1.1967 Acc: 0.424 | Val Loss: 1.7387 Acc: 0.241 | F1: 0.235 UAR: 0.222\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 27/30 | Train Loss: 1.1252 Acc: 0.441 | Val Loss: 1.8512 Acc: 0.219 | F1: 0.242 UAR: 0.227\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 28/30 | Train Loss: 1.1173 Acc: 0.429 | Val Loss: 1.5683 Acc: 0.302 | F1: 0.289 UAR: 0.220\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 29/30 | Train Loss: 1.0757 Acc: 0.418 | Val Loss: 1.4455 Acc: 0.351 | F1: 0.342 UAR: 0.208\n",
      "[aug_no_freq_mask_blocks5_base64] Epoch 30/30 | Train Loss: 1.1045 Acc: 0.449 | Val Loss: 1.3751 Acc: 0.379 | F1: 0.359 UAR: 0.227\n",
      "üèÉ View run aug_no_freq_mask_blocks5_base64 at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4/runs/3b04abb1a3474daf99f56a5d48032fd4\n",
      "üß™ View experiment at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4\n",
      "--> no_freq_mask: best UAR=0.2617\n",
      "\n",
      "=== Running augmentation config: no_time_shift ===\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 1/30 | Train Loss: 8.3207 Acc: 0.219 | Val Loss: 2.8286 Acc: 0.294 | F1: 0.322 UAR: 0.214\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 2/30 | Train Loss: 2.4994 Acc: 0.227 | Val Loss: 4.7295 Acc: 0.158 | F1: 0.168 UAR: 0.207\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 3/30 | Train Loss: 2.6767 Acc: 0.221 | Val Loss: 3.9297 Acc: 0.251 | F1: 0.234 UAR: 0.219\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 4/30 | Train Loss: 2.4896 Acc: 0.230 | Val Loss: 2.3483 Acc: 0.292 | F1: 0.294 UAR: 0.241\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 5/30 | Train Loss: 1.9994 Acc: 0.223 | Val Loss: 2.2953 Acc: 0.112 | F1: 0.117 UAR: 0.223\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 6/30 | Train Loss: 1.7894 Acc: 0.247 | Val Loss: 2.1409 Acc: 0.289 | F1: 0.313 UAR: 0.235\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 7/30 | Train Loss: 1.7459 Acc: 0.249 | Val Loss: 1.7502 Acc: 0.361 | F1: 0.229 UAR: 0.210\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 8/30 | Train Loss: 1.6985 Acc: 0.252 | Val Loss: 1.9980 Acc: 0.221 | F1: 0.182 UAR: 0.225\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 9/30 | Train Loss: 1.7752 Acc: 0.253 | Val Loss: 1.5596 Acc: 0.267 | F1: 0.277 UAR: 0.231\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 10/30 | Train Loss: 1.5650 Acc: 0.272 | Val Loss: 1.4847 Acc: 0.313 | F1: 0.249 UAR: 0.238\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 11/30 | Train Loss: 1.4968 Acc: 0.297 | Val Loss: 1.7516 Acc: 0.228 | F1: 0.246 UAR: 0.248\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 12/30 | Train Loss: 1.4777 Acc: 0.304 | Val Loss: 1.6263 Acc: 0.275 | F1: 0.279 UAR: 0.237\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 13/30 | Train Loss: 1.4631 Acc: 0.304 | Val Loss: 2.0174 Acc: 0.194 | F1: 0.180 UAR: 0.249\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 14/30 | Train Loss: 1.4449 Acc: 0.309 | Val Loss: 1.6337 Acc: 0.361 | F1: 0.300 UAR: 0.233\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 15/30 | Train Loss: 1.4042 Acc: 0.296 | Val Loss: 1.8482 Acc: 0.217 | F1: 0.198 UAR: 0.249\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 16/30 | Train Loss: 1.3976 Acc: 0.324 | Val Loss: 1.5170 Acc: 0.305 | F1: 0.309 UAR: 0.272\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 17/30 | Train Loss: 1.3379 Acc: 0.356 | Val Loss: 1.4888 Acc: 0.323 | F1: 0.336 UAR: 0.242\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 18/30 | Train Loss: 1.3145 Acc: 0.371 | Val Loss: 1.5668 Acc: 0.283 | F1: 0.288 UAR: 0.236\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 19/30 | Train Loss: 1.2883 Acc: 0.379 | Val Loss: 1.6760 Acc: 0.250 | F1: 0.250 UAR: 0.258\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 20/30 | Train Loss: 1.3389 Acc: 0.332 | Val Loss: 1.5587 Acc: 0.258 | F1: 0.284 UAR: 0.217\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 21/30 | Train Loss: 1.2463 Acc: 0.355 | Val Loss: 1.5450 Acc: 0.287 | F1: 0.320 UAR: 0.251\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 22/30 | Train Loss: 1.2140 Acc: 0.387 | Val Loss: 1.6022 Acc: 0.290 | F1: 0.313 UAR: 0.253\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 23/30 | Train Loss: 1.2435 Acc: 0.402 | Val Loss: 1.6160 Acc: 0.277 | F1: 0.303 UAR: 0.227\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 24/30 | Train Loss: 1.2332 Acc: 0.407 | Val Loss: 1.3971 Acc: 0.396 | F1: 0.350 UAR: 0.236\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 25/30 | Train Loss: 1.2025 Acc: 0.383 | Val Loss: 1.8420 Acc: 0.232 | F1: 0.242 UAR: 0.238\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 26/30 | Train Loss: 1.2363 Acc: 0.414 | Val Loss: 1.7901 Acc: 0.197 | F1: 0.222 UAR: 0.251\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 27/30 | Train Loss: 1.1945 Acc: 0.418 | Val Loss: 1.3732 Acc: 0.374 | F1: 0.373 UAR: 0.225\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 28/30 | Train Loss: 1.1667 Acc: 0.445 | Val Loss: 1.6137 Acc: 0.294 | F1: 0.246 UAR: 0.230\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 29/30 | Train Loss: 1.1854 Acc: 0.388 | Val Loss: 1.3983 Acc: 0.347 | F1: 0.363 UAR: 0.232\n",
      "[aug_no_time_shift_blocks5_base64] Epoch 30/30 | Train Loss: 1.1620 Acc: 0.440 | Val Loss: 1.5586 Acc: 0.298 | F1: 0.307 UAR: 0.229\n",
      "üèÉ View run aug_no_time_shift_blocks5_base64 at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4/runs/0d5fe9f5b5714bd6b23ca8e7c8a6102b\n",
      "üß™ View experiment at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4\n",
      "--> no_time_shift: best UAR=0.2723\n",
      "\n",
      "=== Running augmentation config: no_gain ===\n",
      "[aug_no_gain_blocks5_base64] Epoch 1/30 | Train Loss: 10.4965 Acc: 0.208 | Val Loss: 3.3083 Acc: 0.391 | F1: 0.365 UAR: 0.207\n",
      "[aug_no_gain_blocks5_base64] Epoch 2/30 | Train Loss: 2.3558 Acc: 0.216 | Val Loss: 4.2584 Acc: 0.069 | F1: 0.094 UAR: 0.221\n",
      "[aug_no_gain_blocks5_base64] Epoch 3/30 | Train Loss: 2.6780 Acc: 0.221 | Val Loss: 4.2545 Acc: 0.358 | F1: 0.289 UAR: 0.221\n",
      "[aug_no_gain_blocks5_base64] Epoch 4/30 | Train Loss: 2.3423 Acc: 0.237 | Val Loss: 3.1307 Acc: 0.305 | F1: 0.323 UAR: 0.245\n",
      "[aug_no_gain_blocks5_base64] Epoch 5/30 | Train Loss: 2.6950 Acc: 0.222 | Val Loss: 2.7534 Acc: 0.344 | F1: 0.240 UAR: 0.206\n",
      "[aug_no_gain_blocks5_base64] Epoch 6/30 | Train Loss: 2.5095 Acc: 0.238 | Val Loss: 2.9103 Acc: 0.200 | F1: 0.262 UAR: 0.260\n",
      "[aug_no_gain_blocks5_base64] Epoch 7/30 | Train Loss: 2.2355 Acc: 0.245 | Val Loss: 3.1978 Acc: 0.204 | F1: 0.234 UAR: 0.259\n",
      "[aug_no_gain_blocks5_base64] Epoch 8/30 | Train Loss: 2.1224 Acc: 0.252 | Val Loss: 3.4620 Acc: 0.113 | F1: 0.095 UAR: 0.226\n",
      "[aug_no_gain_blocks5_base64] Epoch 9/30 | Train Loss: 1.9613 Acc: 0.278 | Val Loss: 2.4742 Acc: 0.277 | F1: 0.293 UAR: 0.248\n",
      "[aug_no_gain_blocks5_base64] Epoch 10/30 | Train Loss: 1.8544 Acc: 0.271 | Val Loss: 3.9349 Acc: 0.184 | F1: 0.184 UAR: 0.264\n",
      "[aug_no_gain_blocks5_base64] Epoch 11/30 | Train Loss: 1.8391 Acc: 0.270 | Val Loss: 1.9235 Acc: 0.334 | F1: 0.360 UAR: 0.239\n",
      "[aug_no_gain_blocks5_base64] Epoch 12/30 | Train Loss: 1.6181 Acc: 0.297 | Val Loss: 2.0695 Acc: 0.298 | F1: 0.265 UAR: 0.218\n",
      "[aug_no_gain_blocks5_base64] Epoch 13/30 | Train Loss: 1.5767 Acc: 0.310 | Val Loss: 1.8037 Acc: 0.338 | F1: 0.350 UAR: 0.231\n",
      "[aug_no_gain_blocks5_base64] Epoch 14/30 | Train Loss: 1.5466 Acc: 0.301 | Val Loss: 3.5202 Acc: 0.161 | F1: 0.142 UAR: 0.249\n",
      "[aug_no_gain_blocks5_base64] Epoch 15/30 | Train Loss: 1.6259 Acc: 0.293 | Val Loss: 1.9064 Acc: 0.233 | F1: 0.227 UAR: 0.246\n",
      "[aug_no_gain_blocks5_base64] Epoch 16/30 | Train Loss: 1.4155 Acc: 0.297 | Val Loss: 1.9012 Acc: 0.230 | F1: 0.257 UAR: 0.216\n",
      "[aug_no_gain_blocks5_base64] Epoch 17/30 | Train Loss: 1.3986 Acc: 0.317 | Val Loss: 2.2328 Acc: 0.221 | F1: 0.221 UAR: 0.243\n",
      "[aug_no_gain_blocks5_base64] Epoch 18/30 | Train Loss: 1.4152 Acc: 0.341 | Val Loss: 1.4955 Acc: 0.310 | F1: 0.321 UAR: 0.225\n",
      "[aug_no_gain_blocks5_base64] Epoch 19/30 | Train Loss: 1.3196 Acc: 0.320 | Val Loss: 1.5107 Acc: 0.308 | F1: 0.334 UAR: 0.225\n",
      "[aug_no_gain_blocks5_base64] Epoch 20/30 | Train Loss: 1.3330 Acc: 0.326 | Val Loss: 1.6110 Acc: 0.286 | F1: 0.311 UAR: 0.229\n",
      "[aug_no_gain_blocks5_base64] Epoch 21/30 | Train Loss: 1.2573 Acc: 0.346 | Val Loss: 2.1199 Acc: 0.189 | F1: 0.225 UAR: 0.226\n",
      "[aug_no_gain_blocks5_base64] Epoch 22/30 | Train Loss: 1.3278 Acc: 0.352 | Val Loss: 1.4136 Acc: 0.362 | F1: 0.350 UAR: 0.215\n",
      "[aug_no_gain_blocks5_base64] Epoch 23/30 | Train Loss: 1.2469 Acc: 0.374 | Val Loss: 1.5692 Acc: 0.306 | F1: 0.258 UAR: 0.207\n",
      "[aug_no_gain_blocks5_base64] Epoch 24/30 | Train Loss: 1.2659 Acc: 0.364 | Val Loss: 1.8399 Acc: 0.317 | F1: 0.275 UAR: 0.239\n",
      "[aug_no_gain_blocks5_base64] Epoch 25/30 | Train Loss: 1.2107 Acc: 0.361 | Val Loss: 1.5608 Acc: 0.314 | F1: 0.324 UAR: 0.236\n",
      "[aug_no_gain_blocks5_base64] Epoch 26/30 | Train Loss: 1.2436 Acc: 0.409 | Val Loss: 1.5347 Acc: 0.295 | F1: 0.322 UAR: 0.218\n",
      "[aug_no_gain_blocks5_base64] Epoch 27/30 | Train Loss: 1.2251 Acc: 0.371 | Val Loss: 1.7482 Acc: 0.267 | F1: 0.291 UAR: 0.233\n",
      "[aug_no_gain_blocks5_base64] Epoch 28/30 | Train Loss: 1.2016 Acc: 0.365 | Val Loss: 1.6381 Acc: 0.289 | F1: 0.287 UAR: 0.236\n",
      "[aug_no_gain_blocks5_base64] Epoch 29/30 | Train Loss: 1.1768 Acc: 0.370 | Val Loss: 1.4831 Acc: 0.315 | F1: 0.338 UAR: 0.233\n",
      "[aug_no_gain_blocks5_base64] Epoch 30/30 | Train Loss: 1.1608 Acc: 0.399 | Val Loss: 1.5253 Acc: 0.324 | F1: 0.266 UAR: 0.216\n",
      "üèÉ View run aug_no_gain_blocks5_base64 at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4/runs/9764b978b2de43109f5b5ea413665052\n",
      "üß™ View experiment at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4\n",
      "--> no_gain: best UAR=0.2641\n",
      "\n",
      "=== Running augmentation config: no_mixup ===\n",
      "[aug_no_mixup_blocks5_base64] Epoch 1/30 | Train Loss: 5.2057 Acc: 0.221 | Val Loss: 3.3675 Acc: 0.161 | F1: 0.189 UAR: 0.240\n",
      "[aug_no_mixup_blocks5_base64] Epoch 2/30 | Train Loss: 2.8189 Acc: 0.242 | Val Loss: 1.9811 Acc: 0.176 | F1: 0.221 UAR: 0.209\n",
      "[aug_no_mixup_blocks5_base64] Epoch 3/30 | Train Loss: 2.6318 Acc: 0.237 | Val Loss: 2.8869 Acc: 0.300 | F1: 0.217 UAR: 0.215\n",
      "[aug_no_mixup_blocks5_base64] Epoch 4/30 | Train Loss: 2.6225 Acc: 0.252 | Val Loss: 1.6781 Acc: 0.354 | F1: 0.256 UAR: 0.207\n",
      "[aug_no_mixup_blocks5_base64] Epoch 5/30 | Train Loss: 2.3187 Acc: 0.276 | Val Loss: 1.8986 Acc: 0.196 | F1: 0.231 UAR: 0.226\n",
      "[aug_no_mixup_blocks5_base64] Epoch 6/30 | Train Loss: 2.2124 Acc: 0.271 | Val Loss: 2.2255 Acc: 0.226 | F1: 0.257 UAR: 0.253\n",
      "[aug_no_mixup_blocks5_base64] Epoch 7/30 | Train Loss: 1.9061 Acc: 0.296 | Val Loss: 1.9089 Acc: 0.241 | F1: 0.191 UAR: 0.253\n",
      "[aug_no_mixup_blocks5_base64] Epoch 8/30 | Train Loss: 1.8094 Acc: 0.319 | Val Loss: 1.6739 Acc: 0.281 | F1: 0.316 UAR: 0.264\n",
      "[aug_no_mixup_blocks5_base64] Epoch 9/30 | Train Loss: 1.6465 Acc: 0.374 | Val Loss: 1.7959 Acc: 0.211 | F1: 0.257 UAR: 0.268\n",
      "[aug_no_mixup_blocks5_base64] Epoch 10/30 | Train Loss: 1.5769 Acc: 0.369 | Val Loss: 1.5822 Acc: 0.282 | F1: 0.322 UAR: 0.250\n",
      "[aug_no_mixup_blocks5_base64] Epoch 11/30 | Train Loss: 1.4257 Acc: 0.399 | Val Loss: 1.4174 Acc: 0.299 | F1: 0.320 UAR: 0.219\n",
      "[aug_no_mixup_blocks5_base64] Epoch 12/30 | Train Loss: 1.4178 Acc: 0.413 | Val Loss: 1.4738 Acc: 0.405 | F1: 0.292 UAR: 0.216\n",
      "[aug_no_mixup_blocks5_base64] Epoch 13/30 | Train Loss: 1.3166 Acc: 0.455 | Val Loss: 1.7203 Acc: 0.223 | F1: 0.265 UAR: 0.233\n",
      "[aug_no_mixup_blocks5_base64] Epoch 14/30 | Train Loss: 1.2832 Acc: 0.459 | Val Loss: 1.7444 Acc: 0.252 | F1: 0.287 UAR: 0.233\n",
      "[aug_no_mixup_blocks5_base64] Epoch 15/30 | Train Loss: 1.2688 Acc: 0.454 | Val Loss: 1.8567 Acc: 0.217 | F1: 0.230 UAR: 0.238\n",
      "[aug_no_mixup_blocks5_base64] Epoch 16/30 | Train Loss: 1.1736 Acc: 0.504 | Val Loss: 1.7403 Acc: 0.291 | F1: 0.284 UAR: 0.247\n",
      "[aug_no_mixup_blocks5_base64] Epoch 17/30 | Train Loss: 1.1481 Acc: 0.524 | Val Loss: 1.5911 Acc: 0.298 | F1: 0.257 UAR: 0.222\n",
      "[aug_no_mixup_blocks5_base64] Epoch 18/30 | Train Loss: 1.1145 Acc: 0.536 | Val Loss: 1.7801 Acc: 0.224 | F1: 0.260 UAR: 0.226\n",
      "[aug_no_mixup_blocks5_base64] Epoch 19/30 | Train Loss: 1.0991 Acc: 0.539 | Val Loss: 1.8050 Acc: 0.245 | F1: 0.243 UAR: 0.243\n",
      "[aug_no_mixup_blocks5_base64] Epoch 20/30 | Train Loss: 1.1294 Acc: 0.533 | Val Loss: 1.6664 Acc: 0.327 | F1: 0.243 UAR: 0.211\n",
      "[aug_no_mixup_blocks5_base64] Epoch 21/30 | Train Loss: 1.0425 Acc: 0.576 | Val Loss: 2.0685 Acc: 0.218 | F1: 0.240 UAR: 0.245\n",
      "[aug_no_mixup_blocks5_base64] Epoch 22/30 | Train Loss: 1.0344 Acc: 0.574 | Val Loss: 1.8954 Acc: 0.312 | F1: 0.279 UAR: 0.236\n",
      "[aug_no_mixup_blocks5_base64] Epoch 23/30 | Train Loss: 1.1308 Acc: 0.547 | Val Loss: 1.5771 Acc: 0.356 | F1: 0.313 UAR: 0.238\n",
      "[aug_no_mixup_blocks5_base64] Epoch 24/30 | Train Loss: 0.9475 Acc: 0.610 | Val Loss: 1.7022 Acc: 0.286 | F1: 0.323 UAR: 0.225\n",
      "[aug_no_mixup_blocks5_base64] Epoch 25/30 | Train Loss: 0.9080 Acc: 0.634 | Val Loss: 1.6061 Acc: 0.287 | F1: 0.296 UAR: 0.228\n",
      "[aug_no_mixup_blocks5_base64] Epoch 26/30 | Train Loss: 0.9098 Acc: 0.622 | Val Loss: 1.9934 Acc: 0.235 | F1: 0.211 UAR: 0.214\n",
      "[aug_no_mixup_blocks5_base64] Epoch 27/30 | Train Loss: 0.8527 Acc: 0.652 | Val Loss: 1.8772 Acc: 0.241 | F1: 0.264 UAR: 0.230\n",
      "[aug_no_mixup_blocks5_base64] Epoch 28/30 | Train Loss: 0.7961 Acc: 0.679 | Val Loss: 1.7371 Acc: 0.285 | F1: 0.304 UAR: 0.213\n",
      "[aug_no_mixup_blocks5_base64] Epoch 29/30 | Train Loss: 0.7921 Acc: 0.668 | Val Loss: 1.9309 Acc: 0.269 | F1: 0.277 UAR: 0.209\n",
      "[aug_no_mixup_blocks5_base64] Epoch 30/30 | Train Loss: 0.7835 Acc: 0.679 | Val Loss: 1.6312 Acc: 0.331 | F1: 0.346 UAR: 0.230\n",
      "üèÉ View run aug_no_mixup_blocks5_base64 at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4/runs/5108a34f2b9247d9aa2706270e340a77\n",
      "üß™ View experiment at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4\n",
      "--> no_mixup: best UAR=0.2683\n",
      "\n",
      "=== Running augmentation config: no_aug ===\n",
      "[aug_no_aug_blocks5_base64] Epoch 1/30 | Train Loss: 6.4454 Acc: 0.227 | Val Loss: 3.2989 Acc: 0.155 | F1: 0.181 UAR: 0.215\n",
      "[aug_no_aug_blocks5_base64] Epoch 2/30 | Train Loss: 2.9205 Acc: 0.252 | Val Loss: 3.4789 Acc: 0.233 | F1: 0.202 UAR: 0.207\n",
      "[aug_no_aug_blocks5_base64] Epoch 3/30 | Train Loss: 2.7038 Acc: 0.301 | Val Loss: 4.6836 Acc: 0.277 | F1: 0.281 UAR: 0.203\n",
      "[aug_no_aug_blocks5_base64] Epoch 4/30 | Train Loss: 2.2142 Acc: 0.332 | Val Loss: 2.3913 Acc: 0.240 | F1: 0.275 UAR: 0.222\n",
      "[aug_no_aug_blocks5_base64] Epoch 5/30 | Train Loss: 2.0996 Acc: 0.357 | Val Loss: 4.8516 Acc: 0.062 | F1: 0.048 UAR: 0.217\n",
      "[aug_no_aug_blocks5_base64] Epoch 6/30 | Train Loss: 1.7068 Acc: 0.413 | Val Loss: 2.7310 Acc: 0.156 | F1: 0.181 UAR: 0.232\n",
      "[aug_no_aug_blocks5_base64] Epoch 7/30 | Train Loss: 1.5107 Acc: 0.459 | Val Loss: 2.2098 Acc: 0.210 | F1: 0.206 UAR: 0.223\n",
      "[aug_no_aug_blocks5_base64] Epoch 8/30 | Train Loss: 1.3347 Acc: 0.481 | Val Loss: 1.8823 Acc: 0.357 | F1: 0.231 UAR: 0.217\n",
      "[aug_no_aug_blocks5_base64] Epoch 9/30 | Train Loss: 1.1690 Acc: 0.539 | Val Loss: 1.9671 Acc: 0.228 | F1: 0.239 UAR: 0.244\n",
      "[aug_no_aug_blocks5_base64] Epoch 10/30 | Train Loss: 1.0593 Acc: 0.578 | Val Loss: 1.7590 Acc: 0.256 | F1: 0.266 UAR: 0.229\n",
      "[aug_no_aug_blocks5_base64] Epoch 11/30 | Train Loss: 0.9238 Acc: 0.634 | Val Loss: 1.7920 Acc: 0.259 | F1: 0.273 UAR: 0.227\n",
      "[aug_no_aug_blocks5_base64] Epoch 12/30 | Train Loss: 0.8513 Acc: 0.661 | Val Loss: 1.7245 Acc: 0.345 | F1: 0.301 UAR: 0.208\n",
      "[aug_no_aug_blocks5_base64] Epoch 13/30 | Train Loss: 0.8029 Acc: 0.685 | Val Loss: 1.9255 Acc: 0.365 | F1: 0.286 UAR: 0.217\n",
      "[aug_no_aug_blocks5_base64] Epoch 14/30 | Train Loss: 0.8032 Acc: 0.706 | Val Loss: 1.6169 Acc: 0.363 | F1: 0.367 UAR: 0.207\n",
      "[aug_no_aug_blocks5_base64] Epoch 15/30 | Train Loss: 0.7004 Acc: 0.739 | Val Loss: 1.8254 Acc: 0.376 | F1: 0.320 UAR: 0.204\n",
      "[aug_no_aug_blocks5_base64] Epoch 16/30 | Train Loss: 0.7777 Acc: 0.716 | Val Loss: 2.5729 Acc: 0.263 | F1: 0.205 UAR: 0.223\n",
      "[aug_no_aug_blocks5_base64] Epoch 17/30 | Train Loss: 0.6246 Acc: 0.771 | Val Loss: 2.1291 Acc: 0.273 | F1: 0.285 UAR: 0.225\n",
      "[aug_no_aug_blocks5_base64] Epoch 18/30 | Train Loss: 0.4665 Acc: 0.820 | Val Loss: 3.0356 Acc: 0.223 | F1: 0.235 UAR: 0.231\n",
      "[aug_no_aug_blocks5_base64] Epoch 19/30 | Train Loss: 0.5022 Acc: 0.819 | Val Loss: 3.4630 Acc: 0.271 | F1: 0.250 UAR: 0.235\n",
      "[aug_no_aug_blocks5_base64] Epoch 20/30 | Train Loss: 0.8412 Acc: 0.735 | Val Loss: 2.3184 Acc: 0.353 | F1: 0.240 UAR: 0.204\n",
      "[aug_no_aug_blocks5_base64] Epoch 21/30 | Train Loss: 0.4356 Acc: 0.839 | Val Loss: 2.5828 Acc: 0.344 | F1: 0.247 UAR: 0.214\n",
      "[aug_no_aug_blocks5_base64] Epoch 22/30 | Train Loss: 0.4211 Acc: 0.851 | Val Loss: 2.0011 Acc: 0.399 | F1: 0.377 UAR: 0.203\n",
      "[aug_no_aug_blocks5_base64] Epoch 23/30 | Train Loss: 0.2586 Acc: 0.909 | Val Loss: 2.8208 Acc: 0.444 | F1: 0.325 UAR: 0.209\n",
      "[aug_no_aug_blocks5_base64] Epoch 24/30 | Train Loss: 0.3004 Acc: 0.897 | Val Loss: 2.3829 Acc: 0.356 | F1: 0.329 UAR: 0.207\n",
      "[aug_no_aug_blocks5_base64] Epoch 25/30 | Train Loss: 0.2807 Acc: 0.903 | Val Loss: 2.3431 Acc: 0.349 | F1: 0.342 UAR: 0.211\n",
      "[aug_no_aug_blocks5_base64] Epoch 26/30 | Train Loss: 0.2815 Acc: 0.897 | Val Loss: 2.2091 Acc: 0.396 | F1: 0.371 UAR: 0.216\n",
      "[aug_no_aug_blocks5_base64] Epoch 27/30 | Train Loss: 0.2376 Acc: 0.921 | Val Loss: 2.7680 Acc: 0.349 | F1: 0.307 UAR: 0.201\n",
      "[aug_no_aug_blocks5_base64] Epoch 28/30 | Train Loss: 0.2447 Acc: 0.912 | Val Loss: 2.4420 Acc: 0.336 | F1: 0.340 UAR: 0.228\n",
      "[aug_no_aug_blocks5_base64] Epoch 29/30 | Train Loss: 0.1476 Acc: 0.948 | Val Loss: 2.5331 Acc: 0.380 | F1: 0.375 UAR: 0.218\n",
      "[aug_no_aug_blocks5_base64] Epoch 30/30 | Train Loss: 0.2155 Acc: 0.925 | Val Loss: 2.4727 Acc: 0.348 | F1: 0.345 UAR: 0.210\n",
      "üèÉ View run aug_no_aug_blocks5_base64 at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4/runs/bbac79fb2b6f4269aa82b10fa3f08706\n",
      "üß™ View experiment at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4\n",
      "--> no_aug: best UAR=0.2442\n",
      "\n",
      "=== Running augmentation config: mixup_only ===\n",
      "[aug_mixup_only_blocks5_base64] Epoch 1/30 | Train Loss: 8.5891 Acc: 0.214 | Val Loss: 2.7792 Acc: 0.201 | F1: 0.215 UAR: 0.189\n",
      "[aug_mixup_only_blocks5_base64] Epoch 2/30 | Train Loss: 2.4656 Acc: 0.214 | Val Loss: 2.5949 Acc: 0.230 | F1: 0.265 UAR: 0.213\n",
      "[aug_mixup_only_blocks5_base64] Epoch 3/30 | Train Loss: 2.2676 Acc: 0.233 | Val Loss: 2.4863 Acc: 0.101 | F1: 0.143 UAR: 0.197\n",
      "[aug_mixup_only_blocks5_base64] Epoch 4/30 | Train Loss: 2.2235 Acc: 0.240 | Val Loss: 3.0919 Acc: 0.209 | F1: 0.171 UAR: 0.215\n",
      "[aug_mixup_only_blocks5_base64] Epoch 5/30 | Train Loss: 2.1416 Acc: 0.240 | Val Loss: 2.4478 Acc: 0.272 | F1: 0.228 UAR: 0.193\n",
      "[aug_mixup_only_blocks5_base64] Epoch 6/30 | Train Loss: 1.7922 Acc: 0.263 | Val Loss: 2.6251 Acc: 0.144 | F1: 0.175 UAR: 0.229\n",
      "[aug_mixup_only_blocks5_base64] Epoch 7/30 | Train Loss: 1.7645 Acc: 0.271 | Val Loss: 1.7565 Acc: 0.212 | F1: 0.235 UAR: 0.253\n",
      "[aug_mixup_only_blocks5_base64] Epoch 8/30 | Train Loss: 1.6372 Acc: 0.288 | Val Loss: 2.1227 Acc: 0.171 | F1: 0.209 UAR: 0.241\n",
      "[aug_mixup_only_blocks5_base64] Epoch 9/30 | Train Loss: 1.4882 Acc: 0.334 | Val Loss: 1.5999 Acc: 0.298 | F1: 0.302 UAR: 0.237\n",
      "[aug_mixup_only_blocks5_base64] Epoch 10/30 | Train Loss: 1.4639 Acc: 0.314 | Val Loss: 2.3177 Acc: 0.203 | F1: 0.224 UAR: 0.234\n",
      "[aug_mixup_only_blocks5_base64] Epoch 11/30 | Train Loss: 1.4481 Acc: 0.322 | Val Loss: 1.8073 Acc: 0.358 | F1: 0.233 UAR: 0.208\n",
      "[aug_mixup_only_blocks5_base64] Epoch 12/30 | Train Loss: 1.4461 Acc: 0.333 | Val Loss: 1.8588 Acc: 0.359 | F1: 0.365 UAR: 0.232\n",
      "[aug_mixup_only_blocks5_base64] Epoch 13/30 | Train Loss: 1.4740 Acc: 0.333 | Val Loss: 1.4329 Acc: 0.363 | F1: 0.337 UAR: 0.228\n",
      "[aug_mixup_only_blocks5_base64] Epoch 14/30 | Train Loss: 1.4338 Acc: 0.347 | Val Loss: 1.8535 Acc: 0.201 | F1: 0.223 UAR: 0.241\n",
      "[aug_mixup_only_blocks5_base64] Epoch 15/30 | Train Loss: 1.3312 Acc: 0.342 | Val Loss: 1.6208 Acc: 0.322 | F1: 0.338 UAR: 0.246\n",
      "[aug_mixup_only_blocks5_base64] Epoch 16/30 | Train Loss: 1.3696 Acc: 0.375 | Val Loss: 1.8567 Acc: 0.202 | F1: 0.218 UAR: 0.238\n",
      "[aug_mixup_only_blocks5_base64] Epoch 17/30 | Train Loss: 1.2589 Acc: 0.356 | Val Loss: 2.0655 Acc: 0.158 | F1: 0.188 UAR: 0.227\n",
      "[aug_mixup_only_blocks5_base64] Epoch 18/30 | Train Loss: 1.3029 Acc: 0.388 | Val Loss: 1.8541 Acc: 0.320 | F1: 0.251 UAR: 0.231\n",
      "[aug_mixup_only_blocks5_base64] Epoch 19/30 | Train Loss: 1.2256 Acc: 0.399 | Val Loss: 1.4803 Acc: 0.341 | F1: 0.359 UAR: 0.243\n",
      "[aug_mixup_only_blocks5_base64] Epoch 20/30 | Train Loss: 1.1707 Acc: 0.420 | Val Loss: 1.5207 Acc: 0.292 | F1: 0.324 UAR: 0.242\n",
      "[aug_mixup_only_blocks5_base64] Epoch 21/30 | Train Loss: 1.1712 Acc: 0.416 | Val Loss: 1.9213 Acc: 0.300 | F1: 0.282 UAR: 0.251\n",
      "[aug_mixup_only_blocks5_base64] Epoch 22/30 | Train Loss: 1.1771 Acc: 0.402 | Val Loss: 1.6467 Acc: 0.345 | F1: 0.344 UAR: 0.234\n",
      "[aug_mixup_only_blocks5_base64] Epoch 23/30 | Train Loss: 1.1121 Acc: 0.437 | Val Loss: 1.6672 Acc: 0.296 | F1: 0.318 UAR: 0.231\n",
      "[aug_mixup_only_blocks5_base64] Epoch 24/30 | Train Loss: 1.1050 Acc: 0.430 | Val Loss: 1.5706 Acc: 0.422 | F1: 0.332 UAR: 0.229\n",
      "[aug_mixup_only_blocks5_base64] Epoch 25/30 | Train Loss: 1.0656 Acc: 0.431 | Val Loss: 1.4525 Acc: 0.414 | F1: 0.377 UAR: 0.219\n",
      "[aug_mixup_only_blocks5_base64] Epoch 26/30 | Train Loss: 1.0640 Acc: 0.466 | Val Loss: 1.7349 Acc: 0.355 | F1: 0.238 UAR: 0.203\n",
      "[aug_mixup_only_blocks5_base64] Epoch 27/30 | Train Loss: 1.1162 Acc: 0.447 | Val Loss: 1.8275 Acc: 0.375 | F1: 0.277 UAR: 0.212\n",
      "[aug_mixup_only_blocks5_base64] Epoch 28/30 | Train Loss: 0.9995 Acc: 0.434 | Val Loss: 1.7408 Acc: 0.302 | F1: 0.328 UAR: 0.230\n",
      "[aug_mixup_only_blocks5_base64] Epoch 29/30 | Train Loss: 0.9887 Acc: 0.409 | Val Loss: 1.6847 Acc: 0.342 | F1: 0.352 UAR: 0.231\n",
      "[aug_mixup_only_blocks5_base64] Epoch 30/30 | Train Loss: 0.9775 Acc: 0.395 | Val Loss: 1.8178 Acc: 0.335 | F1: 0.348 UAR: 0.252\n",
      "üèÉ View run aug_mixup_only_blocks5_base64 at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4/runs/92bd6954b2a64f15a366b999bd6f5430\n",
      "üß™ View experiment at: https://dagshub.com/arunps12/VisionInfantNet.mlflow/#/experiments/4\n",
      "--> mixup_only: best UAR=0.2533\n",
      "\n",
      "*** Augmentation ablation summary ***\n",
      "all_aug         -> UAR=0.2648\n",
      "no_time_mask    -> UAR=0.2639\n",
      "no_freq_mask    -> UAR=0.2617\n",
      "no_time_shift   -> UAR=0.2723\n",
      "no_gain         -> UAR=0.2641\n",
      "no_mixup        -> UAR=0.2683\n",
      "no_aug          -> UAR=0.2442\n",
      "mixup_only      -> UAR=0.2533\n",
      "\n",
      ">>> Best augmentation config = no_time_shift with UAR=0.2723\n"
     ]
    }
   ],
   "source": [
    "aug_results = {}\n",
    "\n",
    "for name, cfg in AUG_EXPERIMENTS.items():\n",
    "    print(f\"\\n=== Running augmentation config: {name} ===\")\n",
    "\n",
    "    train_dataset = SpectrogramImageDataset(\n",
    "        train_image_paths,\n",
    "        train_labels_idx,\n",
    "        train=True,\n",
    "        aug_cfg=cfg[\"aug_cfg\"],\n",
    "    )\n",
    "\n",
    "    val_dataset = SpectrogramImageDataset(\n",
    "        val_image_paths,\n",
    "        val_labels_idx,\n",
    "        train=False,\n",
    "        aug_cfg=None,\n",
    "    )\n",
    "\n",
    "    run_name = f\"aug_{name}_blocks{NUM_BLOCKS}_base{BASE_CHANNELS}\"\n",
    "\n",
    "    model, best_uar = train_one_cnn_experiment(\n",
    "        num_blocks=NUM_BLOCKS,\n",
    "        base_channels=BASE_CHANNELS,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        num_classes=num_classes,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        lr=LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        epochs=EPOCHS,\n",
    "        run_name=run_name,\n",
    "        MODEL_DIR=\"saved_models\",\n",
    "        init_method=INIT_METHOD,\n",
    "        use_mixup=cfg[\"use_mixup\"],\n",
    "        mixup_alpha=0.4,\n",
    "        aug_cfg_name=name,\n",
    "        use_weighted_sampler=True,                 \n",
    "        sample_weights=sample_weights_tensor, \n",
    "    )\n",
    "\n",
    "    aug_results[name] = best_uar\n",
    "    print(f\"--> {name}: best UAR={best_uar:.4f}\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n*** Augmentation ablation summary ***\")\n",
    "for name, uar in aug_results.items():\n",
    "    print(f\"{name:15s} -> UAR={uar:.4f}\")\n",
    "\n",
    "best_cfg = max(aug_results, key=aug_results.get)\n",
    "print(f\"\\n>>> Best augmentation config = {best_cfg} with UAR={aug_results[best_cfg]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
